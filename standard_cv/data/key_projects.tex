\section{\faFlask}{Key Projects}

  \resumeEntryStart
    \resumeEntryTD
      {Investigating disease by integrating high-dimensional datasets}{\href{https://doi.org/10.1101/2022.04.29.22274267}{Publication}, \href{https://github.com/jackgisby/covid-longitudinal-multi-omics/tree/main/notebooks}{GitHub}}
    \resumeItemListStart
      \resumeItem {Built compelling stories out of complex data using visualisations and clear technical writing.}
      \resumeItem {Presented to both technical and non-technical audiences, including at international conferences.}
      \resumeItem {Identified predictors of disease by modelling (linear models, lasso, random forests) large (>2TB) datasets.}
      \resumeItem {Used unsupervised and network analyses to identify patient sub-types and explore related features.}
    \resumeItemListEnd
  
    \resumeEntryTD
      {Deep learning for identifying anomalous chemicals}{\href{https://github.com/computational-metabolomics/deepmet}{GitHub}}
    \resumeItemListStart
      \resumeItem {Adapted an anomaly detection method in PyTorch to model a unique data type.}
      \resumeItem {Created a Python package with a command-line API for training and applying models to new data.}
      \resumeItem {Automated unit and integration testing, linting and code coverage checks with GitHub Actions.}
    \resumeItemListEnd
    
    \resumeEntryTD
      {A teaching resource for creating reproducible and shareable data pipelines}{\href{https://github.com/ImperialCollegeLondon/ReCoDE_rnaseq_pipeline}{GitHub}}
    \resumeItemListStart
      \resumeItem {Developed a resource that teaches students how to make pipelines for large datasets using best practices.}
      \resumeItem {Automated integration tests and pushed Docker containers with GitHub Actions.}
    \resumeItemListEnd
    
    \resumeEntryTD
      {A cloud ETL pipeline for combining TfL transport logs with weather data}{\href{https://github.com/jackgisby/tfl-bikes-data-pipeline}{GitHub}}
    \resumeItemListStart
      \resumeItem {Used Airflow to coordinate the ingestion and processing of multiple datasets on the Google Cloud Platform.}
      \resumeItem {Implemented automated PySpark Dataproc jobs to transform and load data to BigQuery.}
      \resumeItem {Modelled data using a star schema and time-based partitioning for efficient analytical queries.}
    \resumeItemListEnd
    
  \resumeEntryEnd
